{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "import subprocess\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "sys.path.append(\"../utls\")\n",
    "sys.path.append(\"../utls\")\n",
    "sys.path.append(\"../.\")\n",
    "sys.path.append(\"../models\")\n",
    "from collections import Counter\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utls.preprocessing import *\n",
    "from utls.postprocessing import get_dataset, generate_hist_df\n",
    "from InterfaceDeclaration import LPBFInterface\n",
    "from models.MLUtls import fade_in_out, standardize_tensor, getKFoldCrossValidationIndexes, train_log, transform_ft, dataset_by_cross_validation, labels_by_classes, get_current_fold_and_hist, LPBFDataset\n",
    "from models.MLModels import SVMModel, CNN_Base_1D_Model, ResNet15_1D_Model\n",
    "\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "\n",
    "project_name = [\"MuSIC\", \"MaPS\", \"MuSIC_EXP1\"]\n",
    "if os.name == \"posix\":\n",
    "    data_dir = subprocess.getoutput(\"echo $DATADIR\")\n",
    "elif os.name == \"nt\":\n",
    "    data_dir = subprocess.getoutput(\"echo %datadir%\")\n",
    "music_dir = os.path.join(data_dir, \"MuSIC\")\n",
    "if not os.path.exists(music_dir):\n",
    "    project_name[0] = \"2024-MUSIC\"\n",
    "daq_dir = os.path.join(data_dir, *project_name, \"Acoustic Monitoring\")\n",
    "lmq_dir = os.path.join(data_dir, *project_name, \"LMQ Monitoring\")\n",
    "del music_dir\n",
    "\n",
    "sampling_rate_daq: int = int(1.25 * 1e6)\n",
    "sampling_rate_lmq: int = int(0.1 * 1e6)\n",
    "tdms_daq_list = natsorted(\n",
    "    [i for i in os.listdir(daq_dir) if i.split(\".\")[-1] == \"tdms\"]\n",
    ")\n",
    "bin_lmq_list = natsorted([i for i in os.listdir(lmq_dir) if i.split(\".\")[-1] == \"bin\"])\n",
    "lmq_channel_name = [\n",
    "    \"Vector ID\",\n",
    "    \"meltpooldiode\",\n",
    "    \"X Coordinate\",\n",
    "    \"Y Coordinate\",\n",
    "    \"Laser power\",\n",
    "    \"Spare\",\n",
    "    \"Laser diode\",\n",
    "    \"Varioscan(focal length)\",\n",
    "]\n",
    "process_regime = [\n",
    "    [0,    59, \"Base\"  ], \n",
    "    [60,  129, \"KH\"    ], \n",
    "    [130, 199, \"Normal\"], \n",
    "    [200, 269, \"RLoF\"  ], \n",
    "    [269, 339, \"LoF\"   ] \n",
    "]\n",
    "\n",
    "dataset,sc_power,le_direction,le_speed,le_region = get_dataset()\n",
    "\n",
    "with open(os.path.join(\"../\",'outputs',\"intermediate\",f\"sc_power.pkl\"), 'rb') as handle:\n",
    "    sc_power = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(\"../\",'outputs',\"intermediate\",f\"le_speed.pkl\"), 'rb') as handle:\n",
    "    le_speed = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(\"../\",'outputs',\"intermediate\",f\"le_region.pkl\"), 'rb') as handle:\n",
    "    le_region = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(\"../\",'outputs',\"intermediate\",f\"le_direction.pkl\"), 'rb') as handle:\n",
    "    le_direction = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=50\n",
    "his_dir = os.path.join(os.pardir,\"lfs\",\"weights/hist\")\n",
    "snap_dir = os.path.join(os.pardir,\"lfs\",\"weights/\")\n",
    "model_name= [\"CNN\"]\n",
    "acoustic_type =['ae','mic'] \n",
    "context_type = []\n",
    "# context_type = ['energy']\n",
    "output_type = ['direction']\n",
    "folds = 10\n",
    "\n",
    "df = generate_hist_df(his_dir,model_name,acoustic_type,context_type,output_type,folds,max_epochs)\n",
    "df['Input Type'] = df['Input type'].str.replace('+', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (in %): {4: 22.366357575416387, 0: 22.381637888743235, 1: 27.49973862621941, 3: 27.572119057767626, 2: 0.18014685185334117}\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for _cube_position, _laser_power, _scanning_speed, _regime_info, _print_direction, _mic, _ae, _defect_labels in data_loader:  # Iterate over the dataset\n",
    "    targets = _print_direction\n",
    "    labels.extend(targets.numpy())  # Collect all labels (assumes targets are a tensor)\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(labels)\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "# Display the distribution as percentages\n",
    "class_distribution = {cls: count / total_samples * 100 for cls, count in class_counts.items()}\n",
    "print(\"Class Distribution (in %):\", class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 11777])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ae.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,snap_list = read_trained_model(snap_dir,model_name,acoustic_type,context_type,output_type,folds,max_epochs)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_name= \"CNN\"\n",
    "_inputs = \"ae\"\n",
    "_outputs = \"direction\"\n",
    "meta_list = []\n",
    "pred_list = []\n",
    "label_list = [] \n",
    "ae_cf_direction = get_confusion_matrix(model, 'CNN','ae', 'direction')\n",
    "mic_cf_direction = get_confusion_matrix(model, 'CNN','mic','direction')\n",
    "ae_cf_position = get_confusion_matrix(model, 'CNN','ae', 'position')\n",
    "mic_cf_position = get_confusion_matrix(model, 'CNN','mic','position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"../outputs/intermediate/ae_cf_direction.npy\",ae_cf_direction)\n",
    "np.save(f\"../outputs/intermediate/mic_cf_direction.npy\",mic_cf_direction)\n",
    "np.save(f\"../outputs/intermediate/ae_cf_position.npy\",ae_cf_position)\n",
    "np.save(f\"../outputs/intermediate/mic_cf_position.npy\",mic_cf_position)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
